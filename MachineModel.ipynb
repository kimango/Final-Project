{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import sklearn as skl\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import os\r\n",
    "# Find the latest version of spark 3.0 from http://www.apache.org/dist/spark/ and enter as the spark version\r\n",
    "# For example:\r\n",
    "# spark_version = 'spark-3.0.3'\r\n",
    "spark_version = 'spark-3.0.3'\r\n",
    "os.environ['SPARK_VERSION']=spark_version\r\n",
    "\r\n",
    "# Install Spark and Java\r\n",
    "!apt-get update\r\n",
    "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\r\n",
    "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\r\n",
    "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\r\n",
    "!pip install -q findspark\r\n",
    "\r\n",
    "# Set Environment Variables\r\n",
    "import os\r\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\r\n",
    "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\r\n",
    "\r\n",
    "# Start a SparkSession\r\n",
    "import findspark\r\n",
    "findspark.init()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "'apt-get' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "The system cannot find the path specified.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "tar: Error opening archive: Failed to open '$SPARK_VERSION-bin-hadoop2.7.tgz'\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "Exception",
     "evalue": "Unable to find py4j, your SPARK_HOME may not be configured correctly",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonData\\lib\\site-packages\\findspark.py\u001b[0m in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m         \u001b[0mpy4j\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"lib\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"py4j-*.zip\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10896/3977266261.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# Start a SparkSession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mfindspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonData\\lib\\site-packages\\findspark.py\u001b[0m in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m         raise Exception(\n\u001b[1;32m--> 146\u001b[1;33m             \u001b[1;34m\"Unable to find py4j, your SPARK_HOME may not be configured correctly\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m         )\n\u001b[0;32m    148\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Unable to find py4j, your SPARK_HOME may not be configured correctly"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Download the Postgres driver that will allow Spark to interact with Postgres.\r\n",
    "!wget https://jdbc.postgresql.org/download/postgresql-42.2.16.jar"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Start Spark session\r\n",
    "from pyspark.sql import SparkSession\r\n",
    "from pyspark.sql.functions import substring, length, col, expr, to_timestamp, date_format, round\r\n",
    "\r\n",
    "spark = SparkSession.builder.appName(\"LMPT-Forest-Fires\").config(\"spark.driver.extraClassPath\",\"/content/postgresql-42.2.16.jar\").getOrCreate()\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "connection_string = 'lmpt-finalproject.coke2w4vs8wf.us-east-2.rds.amazonaws.com'\r\n",
    "password = 'LMPTp4ssw0rd' \r\n",
    "database_name = 'postgres'\r\n",
    "\r\n",
    "# Configure settings for RDS\r\n",
    "mode = \"append\"\r\n",
    "jdbc_url=f\"jdbc:postgresql://{connection_string}:5432/{database_name}\"\r\n",
    "config = {\"user\":\"postgres\", \r\n",
    "          \"password\": password, \r\n",
    "          \"driver\":\"org.postgresql.Driver\"}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "# Read in data \r\n",
    "df = spark.read.jdbc(jdbc_url,table='fires_2006to2018',properties=config)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create new dataframe for model with chosen features\r\n",
    "fire_df = df[['calendar_year','fire_start_date', 'fire_fighting_start_size', 'bh_fs_date', 'bh_hectares', 'weather_conditions_over_fire', 'true_cause']]  \r\n",
    "# The following features have been witheld 'fire_number','size_class', 'start_for_fire_date',,\r\n",
    "fire_df.show(10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Drop off NA  starting sizes and convert to data type double\r\n",
    "fire_df = fire_df[fire_df.fire_fighting_start_size != 'NA']\r\n",
    "fire_df = fire_df.withColumn('fire_fighting_start_size',fire_df['fire_fighting_start_size'].cast(\"double\"))\r\n",
    "fire_df.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Replace bh_hectares with ratio of fire size between \"start\" and \"being held\". Ratio of fire size will be the final predictive variable\r\n",
    "fire_df = fire_df.withColumn(\"fire_growth\",col(\"bh_hectares\")/col(\"fire_fighting_start_size\")).drop(\"bh_hectares\")\r\n",
    "fire_df.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Review data types\r\n",
    "fire_df.dtypes"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Check Null Values in Columns\r\n",
    "#Dict_Null = {col:fire_df.filter(df[col].isNull()).count() for col in fire_df.columns}\r\n",
    "Dict_Null = {col:fire_df.filter(fire_df[col].isNull()).count() for col in fire_df.columns}\r\n",
    "Dict_Null"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Count rows of data \r\n",
    "fire_df.count()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Replace Null Values \r\n",
    "fire_df = fire_df.na.fill(\"unknown\")\r\n",
    "fire_df.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fire_df.show(10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 1. Removing last 3 digits from fire_number to to identify location area of fire\r\n",
    "\r\n",
    "#fire_df = fire_df.withColumn(\"fire_number\",expr(\"substring(fire_number, 1, length(fire_number)-3)\"))\r\n",
    "#fire_df.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 2.Convert discovered_date to look at just months (find trends in months/ seasons)\r\n",
    "from pyspark.sql.functions import to_timestamp, date_format\r\n",
    "fire_df = fire_df.withColumn('fire_start_date', to_timestamp (col('fire_start_date'))).withColumn('Month', date_format(col('fire_start_date'), 'M'))\r\n",
    "fire_df.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 3. Convert start_for_fire_date & bh_fs_date to timestamp to find length of fire held time\r\n",
    "#fire_df = fire_df.withColumn('start_for_fire_date', col('start_for_fire_date').cast('timestamp'))\r\n",
    "fire_df = fire_df.withColumn('fire_start_date', col('fire_start_date').cast('timestamp'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Convert start_for_fire_date & bh_fs_date to timestamp to find length of fire held time\r\n",
    "fire_df = fire_df.withColumn('bh_fs_date', col('bh_fs_date').cast('timestamp'))\r\n",
    "fire_df.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Check dates have been converted to timestamps\r\n",
    "fire_df.dtypes"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Find the length of fire held \r\n",
    "fire_df = fire_df.withColumn(\"bh_fs_date\",to_timestamp(col(\"bh_fs_date\"),\"HH:mm:ss.SSS\")) \\\r\n",
    "   .withColumn(\"fire_start_date\",to_timestamp(col(\"fire_start_date\"),\"HH:mm:ss.SSS\")) \\\r\n",
    "   .withColumn(\"DiffInSeconds\", col(\"bh_fs_date\").cast(\"long\") - col(\"fire_start_date\").cast(\"long\")) \\\r\n",
    "   .withColumn(\"DiffInMinutes\",(col(\"DiffInSeconds\")/60)) \\\r\n",
    "   .withColumn(\"DiffInHours\",(col(\"DiffInSeconds\")/3600)) \r\n",
    "fire_df.show(truncate=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#import required libraries\r\n",
    "from pyspark.ml.feature import StringIndexer\r\n",
    "weather_conditions_over_fire_indexer = StringIndexer(inputCol=\"weather_conditions_over_fire\", outputCol=\"weather_conditions_over_fireIndex\")\r\n",
    "\r\n",
    "#Use one hot encoding to encode catergoratical columns \r\n",
    "ec_df = weather_conditions_over_fire_indexer.fit(fire_df).transform(fire_df)\r\n",
    "ec_df.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Use one hot encoding to encode catergoratical columns \r\n",
    "true_cause_indexer = StringIndexer(inputCol=\"true_cause\", outputCol=\"true_cause_fireIndex\")\r\n",
    "\r\n",
    "#Fits a model to the input dataset with optional parameters.\r\n",
    "ec_df1 = true_cause_indexer.fit(fire_df).transform(fire_df)\r\n",
    "ec_df1.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#import module\r\n",
    "from pyspark.ml import Pipeline\r\n",
    "\r\n",
    "#Create pipeline and pass all stages\r\n",
    "pipeline = Pipeline(stages=[weather_conditions_over_fire_indexer,\r\n",
    "                            true_cause_indexer,\r\n",
    "                    ])\r\n",
    "\r\n",
    "                    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create pipeline to pass all stages \r\n",
    "df_transformed = pipeline.fit(fire_df).transform(fire_df)\r\n",
    "df_transformed.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Clean dataframe for model \r\n",
    "final_df = df_transformed.drop(\"weather_conditions_over_fire\").drop(\"true_cause\").drop(\"Month\").drop(\"DiffInSeconds\").drop(\"DiffInMinutes\")\\\r\n",
    ".drop(\"fire_start_date\").drop(\"bh_fs_date\")\r\n",
    "final_df.show(10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# dropping all infinity and NaN values before hitting the database\r\n",
    "final_df = final_df.replace([np.inf, -np.inf], np.nan)\r\n",
    "final_df = final_df[final_df.fire_growth != np.nan]\r\n",
    "final_df = final_df[final_df.DiffInHours != np.nan]\r\n",
    "final_df = final_df[final_df.fire_fighting_start_size != np.nan]\r\n",
    "final_df.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Decide on features and label: \r\n",
    "#\"user-input\" for how many days before a fire is predicted, we estimate the size before and the fire size is being held i.e. predicting change in starting fire size and discovered size by the number of days \r\n",
    "\r\n",
    "# Split our preprocessed data into our features and target arrays\r\n",
    "# Output labels \r\n",
    "y = final_df.select(\"fire_growth\").toPandas()\r\n",
    "\r\n",
    "# Features data \r\n",
    "X = final_df.drop(\"fire_growth\").toPandas()\r\n",
    "\r\n",
    "# Split the preprocessed data into a training & test dataset\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Check the shape of X \r\n",
    "X.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Check y shape samples\r\n",
    "y.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Define the linear regression model\r\n",
    "from sklearn.linear_model import LinearRegression\r\n",
    "model = LinearRegression()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create a StandardScaler instance\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "scaler = StandardScaler()\r\n",
    "\r\n",
    "# Fit the StandardScaler\r\n",
    "X_scaler = scaler.fit(X_train)\r\n",
    "\r\n",
    "# Scale the data \r\n",
    "X_trained_scaled = X_scaler.transform(X_train)\r\n",
    "X_test_scaled = X_scaler.transform(X_test)\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Train the model\r\n",
    "model.fit(X_train, y_train"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Evaluate the model\r\n",
    "y_pred = model.predict(X_test_scaled)\r\n",
    "y_pred"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Retrieving the model intercept and slope \r\n",
    "print(model.coef_)\r\n",
    "print(model.intercept_)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Calculating the R squared value \r\n",
    "from sklearn.metrics import r2_score\r\n",
    "r2_score(y_test, y_pred)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "# Determine the shape of our training and testing sets.\r\n",
    "# X & y_train are 75% & X & y_test are 25%\r\n",
    "print(X_train.shape)\r\n",
    "print(X_test.shape)\r\n",
    "print(y_train.shape)\r\n",
    "print(y_test.shape)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('PythonData': conda)"
  },
  "interpreter": {
   "hash": "4d4129133cd18cad01b2f7ee89b43a0a40294f4e562846c89a448643ee544a31"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}